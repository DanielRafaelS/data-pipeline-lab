# Data Pipeline Lab

## Vis√£o Geral

Este projeto implementa um pipeline completo de dados utilizando a arquitetura **Medallion (Bronze ‚Üí Silver ‚Üí Gold)** com:

- Python
- PostgreSQL
- Prefect (orquestra√ß√£o)
- Docker Compose

O pipeline extrai dados de uma API p√∫blica de e-commerce, processa e transforma as informa√ß√µes em camadas organizadas e entrega um **modelo dimensional em formato Star Schema**, pronto para consumo anal√≠tico.

---

## Objetivos do Projeto

- Construir um pipeline ETL resiliente e idempotente
- Organizar os dados em camadas seguindo boas pr√°ticas
- Garantir integridade referencial no Data Warehouse
- Disponibilizar um modelo dimensional para an√°lises
- Permitir execu√ß√£o completa via Docker

---

## Arquitetura

O pipeline segue o padr√£o Medallion:

### Bronze (Raw)
- Ingest√£o de dados brutos em JSON
- Armazenamento fiel ao formato da API
- Upsert para evitar duplicidades

### Silver (Tratamento e Estrutura√ß√£o)
- Normaliza√ß√£o dos dados
- Tipagem expl√≠cita
- Tratamento de campos
- Enriquecimentos b√°sicos

### Gold (Modelo Dimensional)
- Modelo Star Schema
- Uso de chaves substitutas (surrogate keys)
- Integridade referencial via foreign keys
- Otimizado para consultas anal√≠ticas

#### Fato
- `fact_sales`

#### Dimens√µes
- `dim_user`
- `dim_product`
- `dim_date`

---

## üìä Modelo Dimensional

O modelo dimensional foi estruturado em formato Star Schema.

![Modelo Dimensional](docs/dimensional_model.png)

---

## Estrat√©gia de Carga

O pipeline suporta:

- Full Refresh
- Carga Incremental (padr√£o)

A carga incremental utiliza `ON CONFLICT` para garantir:

- Idempot√™ncia
- Reprocessamento seguro
- Aus√™ncia de duplicidades

## Setup e Execu√ß√£o

### Pr√©-requisitos

- Docker
- Docker Compose
- Make (opcional, mas recomendado)

---
## Subindo o Ambiente Completo

Para subir PostgreSQL + Prefect Server + Prefect Worker:

```bash
make up
```

Ou manualmente:

```bash
docker compose -f docker-compose.win.yml up -d
```

Ap√≥s subir os containers, a interface do Prefect estar√° dispon√≠vel em:

```
http://localhost:4200
```

---

## Executando o Pipeline

### 1 - Execu√ß√£o local (sem deployment)

Executa o pipeline diretamente via Python (√∫til para desenvolvimento):

```bash
make run
```

Ou manualmente:

```bash
python -m etl
```

---

### 2 - Criando / Atualizando o Deployment no Prefect

Para registrar o pipeline no Prefect:

```bash
make deploy
```

Ou manualmente:

```bash
prefect deploy
```

Selecione o deployment existente (ex: `medallion-etl-local`)  

---

### 3 - Executando via Prefect

Ap√≥s o deploy, √© poss√≠vel executar:

Pela interface web:

```
http://localhost:4200
```

Ou via CLI:

```bash
prefect deployment run "medallion-etl/medallion-etl-local"
```

---

## Derrubando o Ambiente

```bash
make down
```

Ou manualmente:

```bash
docker compose -f docker-compose.win.yml down
```

## üîç Validando os Dados (Analytics)

Ap√≥s a execu√ß√£o do pipeline, voc√™ pode validar os resultados e executar consultas anal√≠ticas diretamente no Data Warehouse.

O arquivo `sql/analytics_examples.sql` cont√©m exemplos de queries sobre o modelo dimensional (camada **Gold**), incluindo consultas na `fact_sales`, `dim_user`, `dim_product` e `dim_date`.

